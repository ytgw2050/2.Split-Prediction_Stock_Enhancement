{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97950b8c",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a30595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Conv2D, Conv2DTranspose, Dense, Flatten, \n",
    "                                    Dropout, BatchNormalization, Reshape, LeakyReLU, LSTM)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca4cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def remove_all_file(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        shutil.rmtree(filepath, ignore_errors=True)\n",
    "        os.makedirs(filepath)\n",
    "    else:\n",
    "        os.makedirs(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74a7e7",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f0fd08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stock_data/image_train/outputSubdivision/oneDayChart'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[1;32m      3\u001b[0m import_train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)  \u001b[38;5;66;03m# 이미지 데이터 전처리\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m import_training_set_oneDayChart \u001b[38;5;241m=\u001b[39m import_train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstock_data/image_train/outputSubdivision/oneDayChart\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m                                                 shuffle\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m                                                 seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m17\u001b[39m,\n\u001b[1;32m     13\u001b[0m                                                 batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m226088\u001b[39m,\n\u001b[1;32m     14\u001b[0m                                                  target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m44\u001b[39m),\n\u001b[1;32m     15\u001b[0m                                                  class_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m import_test_set_oneDayChart \u001b[38;5;241m=\u001b[39m import_train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstock_data/image_test/outputSubdivision/oneDayChart\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m                                                 shuffle\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m                                                 seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m,\n\u001b[1;32m     23\u001b[0m                                                 batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m53859\u001b[39m,\n\u001b[1;32m     24\u001b[0m                                                  target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m44\u001b[39m),\n\u001b[1;32m     25\u001b[0m                                                  class_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/kaist/lib/python3.11/site-packages/keras/src/preprocessing/image.py:1648\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1564\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1579\u001b[0m ):\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m \n\u001b[1;32m   1582\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;124;03m            and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DirectoryIterator(\n\u001b[1;32m   1649\u001b[0m         directory,\n\u001b[1;32m   1650\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1651\u001b[0m         target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[1;32m   1652\u001b[0m         color_mode\u001b[38;5;241m=\u001b[39mcolor_mode,\n\u001b[1;32m   1653\u001b[0m         keep_aspect_ratio\u001b[38;5;241m=\u001b[39mkeep_aspect_ratio,\n\u001b[1;32m   1654\u001b[0m         classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[1;32m   1655\u001b[0m         class_mode\u001b[38;5;241m=\u001b[39mclass_mode,\n\u001b[1;32m   1656\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[1;32m   1657\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1658\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m   1659\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m   1660\u001b[0m         save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[1;32m   1661\u001b[0m         save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[1;32m   1662\u001b[0m         save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[1;32m   1663\u001b[0m         follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[1;32m   1664\u001b[0m         subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[1;32m   1665\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[1;32m   1666\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m   1667\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/kaist/lib/python3.11/site-packages/keras/src/preprocessing/image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    562\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(directory)):\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    565\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stock_data/image_train/outputSubdivision/oneDayChart'"
     ]
    }
   ],
   "source": [
    "# 추후 kb api 통해 데이터 전처리\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import_train_datagen = ImageDataGenerator(rescale = 1./255)  # 이미지 데이터 전처리\n",
    "\n",
    "\n",
    "                                   \n",
    "\n",
    "\n",
    "\n",
    "import_training_set_oneDayChart = import_train_datagen.flow_from_directory(\"stock_data/image_train/outputSubdivision/oneDayChart\",\n",
    "                                                shuffle= True,\n",
    "                                                seed = 17,\n",
    "                                                batch_size = 226088,\n",
    "                                                 target_size=(28,44),\n",
    "                                                 class_mode = 'input')\n",
    "                \n",
    "                                   \n",
    "\n",
    "\n",
    "import_test_set_oneDayChart = import_train_datagen.flow_from_directory(\"stock_data/image_test/outputSubdivision/oneDayChart\",\n",
    "                                                shuffle= True,\n",
    "                                                seed = 7,\n",
    "                                                batch_size = 53859,\n",
    "                                                 target_size=(28, 44),\n",
    "                                                 class_mode = 'input')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a7cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "trainX_oneDayChart = import_training_set_oneDayChart[0][0]\n",
    "#trainy_day = import_training_set_day[0][1]\n",
    "\n",
    "print(1)\n",
    "\n",
    "\n",
    "\n",
    "testX_oneDayChart = import_test_set_oneDayChart[0][0]\n",
    "#testy_day = import_test_set_day[0][1] \n",
    "\n",
    "print(1)\n",
    "\n",
    "\n",
    "train_data_oneDayChart = np.reshape(trainX_oneDayChart, (trainX_oneDayChart.shape))\n",
    "test_data_oneDayChart = np.reshape(testX_oneDayChart, (testX_oneDayChart.shape))\n",
    "\n",
    "print(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817eccd",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b222c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_21 (InputLayer)       [(None, 28, 44, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 28, 44, 64)        1792      \n",
      "                                                                 \n",
      " batch_normalization_56 (Bat  (None, 28, 44, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_56 (LeakyReLU)  (None, 28, 44, 64)        0         \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 14, 22, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_57 (Bat  (None, 14, 22, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_57 (LeakyReLU)  (None, 14, 22, 128)       0         \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 7, 11, 256)        295168    \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 7, 11, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 7, 11, 256)        0         \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 4, 6, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 4, 6, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 4, 6, 512)         0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 12288)             0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 4)                 49156     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,603,972\n",
      "Trainable params: 1,602,052\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 39424)             197120    \n",
      "                                                                 \n",
      " reshape_7 (Reshape)         (None, 7, 11, 512)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_35 (Conv2D  (None, 14, 22, 512)      2359808   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_60 (Bat  (None, 14, 22, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_60 (LeakyReLU)  (None, 14, 22, 512)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_36 (Conv2D  (None, 28, 44, 256)      1179904   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_61 (Bat  (None, 28, 44, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_61 (LeakyReLU)  (None, 28, 44, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_37 (Conv2D  (None, 56, 88, 128)      295040    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_62 (Bat  (None, 56, 88, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_62 (LeakyReLU)  (None, 56, 88, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_38 (Conv2D  (None, 56, 88, 64)       73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 56, 88, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_63 (LeakyReLU)  (None, 56, 88, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_39 (Conv2D  (None, 56, 88, 3)        1731      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,111,235\n",
      "Trainable params: 4,109,315\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow  as tf\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "#     input_shape = (train_data_oneDayChart.shape)\n",
    "    batch_size = 32\n",
    "    kernel_size = 3\n",
    "    latent_dim = 256\n",
    "    layer_filters = [64, 128, 256]\n",
    "\n",
    "    encoder_input = Input(shape=(28,44,3))\n",
    "\n",
    "    # 28 X 28\n",
    "    x = Conv2D(64, 3, padding='same')(encoder_input) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x) \n",
    "\n",
    "    # 28 X 28 -> 14 X 14\n",
    "    x = Conv2D(128, 3, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x) \n",
    "    x = LeakyReLU()(x) \n",
    "\n",
    "    # 14 X 14 -> 7 X 7\n",
    "    x = Conv2D(256, 3, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    # 17 X 7\n",
    "    x = Conv2D(512, 3,strides=2,  padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # 2D 좌표로 표기하기 위하여 2를 출력값으로 지정합니다.\n",
    "    encoder_output = Dense(4)(x)\n",
    "\n",
    "\n",
    "\n",
    "    # Input으로는 2D 좌표가 들어갑니다.\n",
    "    decoder_input = Input(shape=(4, ))\n",
    "\n",
    "\n",
    "    x = Dense(7*11*512)(decoder_input)\n",
    "    x = Reshape( (7,11, 512))(x)\n",
    "\n",
    "    # 7 X 7 -> 7 X 7\n",
    "    x = Conv2DTranspose(512, 3, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    # 7 X 7 -> 14 X 14\n",
    "    x = Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    # 14 X 14 -> 28 X 28\n",
    "    x = Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    # 28 X 28 -> 28 X 28\n",
    "    x = Conv2DTranspose(64, 3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    # 최종 output\n",
    "    decoder_output = Conv2DTranspose(3, 3, strides=1, padding='same', activation='tanh')(x)\n",
    "\n",
    "    encoder_day = Model(encoder_input, encoder_output)\n",
    "    decoder_day = Model(decoder_input, decoder_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    encoder_in = Input(shape=(28, 44, 3))\n",
    "    LEARNING_RATE = 0.0005\n",
    "    BATCH_SIZE = 32\n",
    "    x = encoder_day(encoder_in)\n",
    "    decoder_out = decoder_day(x)\n",
    "\n",
    "\n",
    "    auto_encoder_day = Model(encoder_in, decoder_out)\n",
    "    encoder_day.summary()\n",
    "    decoder_day.summary()\n",
    "    auto_encoder_day.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss=tf.keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fed7d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'models/small_autoencoder_check_point.ckpt'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=True, \n",
    "                             monitor='loss', \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8953e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "# NVDIA GPU 작동 설정 - 메모리 할당 관련 코드\n",
    "#NVIDIA GPU가 있지 않다면 코드 절대 실행 X\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') \n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45105370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "3533/3533 [==============================] - 669s 189ms/step - loss: 0.0060 - val_loss: 3.8251e-04\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00598, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 2/55\n",
      "3533/3533 [==============================] - 704s 199ms/step - loss: 2.1312e-04 - val_loss: 1.3591e-04\n",
      "\n",
      "Epoch 00002: loss improved from 0.00598 to 0.00021, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 3/55\n",
      "3533/3533 [==============================] - 707s 200ms/step - loss: 1.3657e-04 - val_loss: 1.5089e-04\n",
      "\n",
      "Epoch 00003: loss improved from 0.00021 to 0.00014, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 4/55\n",
      "3533/3533 [==============================] - 736s 208ms/step - loss: 1.0477e-04 - val_loss: 8.5706e-05\n",
      "\n",
      "Epoch 00004: loss improved from 0.00014 to 0.00010, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 5/55\n",
      "3533/3533 [==============================] - 728s 206ms/step - loss: 7.7931e-05 - val_loss: 6.2618e-05\n",
      "\n",
      "Epoch 00005: loss improved from 0.00010 to 0.00008, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 6/55\n",
      "3533/3533 [==============================] - 722s 204ms/step - loss: 7.1113e-05 - val_loss: 5.4458e-05\n",
      "\n",
      "Epoch 00006: loss improved from 0.00008 to 0.00007, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 7/55\n",
      "3533/3533 [==============================] - 721s 204ms/step - loss: 5.5046e-05 - val_loss: 4.6986e-05\n",
      "\n",
      "Epoch 00007: loss improved from 0.00007 to 0.00006, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 8/55\n",
      "3533/3533 [==============================] - 722s 204ms/step - loss: 5.3655e-05 - val_loss: 4.4279e-05\n",
      "\n",
      "Epoch 00008: loss improved from 0.00006 to 0.00005, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 9/55\n",
      "3533/3533 [==============================] - 721s 204ms/step - loss: 4.5366e-05 - val_loss: 4.0616e-05\n",
      "\n",
      "Epoch 00009: loss improved from 0.00005 to 0.00005, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 10/55\n",
      "3533/3533 [==============================] - 712s 201ms/step - loss: 4.1838e-05 - val_loss: 5.2543e-05\n",
      "\n",
      "Epoch 00010: loss improved from 0.00005 to 0.00004, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 11/55\n",
      "3533/3533 [==============================] - 720s 204ms/step - loss: 3.8238e-05 - val_loss: 4.1430e-05\n",
      "\n",
      "Epoch 00011: loss improved from 0.00004 to 0.00004, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 12/55\n",
      "3533/3533 [==============================] - 719s 203ms/step - loss: 2.9408e-05 - val_loss: 3.2989e-05\n",
      "\n",
      "Epoch 00012: loss improved from 0.00004 to 0.00003, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 13/55\n",
      "3533/3533 [==============================] - 718s 203ms/step - loss: 3.2613e-05 - val_loss: 2.3824e-05\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.00003\n",
      "Epoch 14/55\n",
      "3533/3533 [==============================] - 719s 204ms/step - loss: 2.8359e-05 - val_loss: 4.3304e-05\n",
      "\n",
      "Epoch 00014: loss improved from 0.00003 to 0.00003, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 15/55\n",
      "3533/3533 [==============================] - 718s 203ms/step - loss: 2.6196e-05 - val_loss: 2.2062e-05\n",
      "\n",
      "Epoch 00015: loss improved from 0.00003 to 0.00003, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 16/55\n",
      "3533/3533 [==============================] - 709s 201ms/step - loss: 2.4647e-05 - val_loss: 6.7659e-04\n",
      "\n",
      "Epoch 00016: loss improved from 0.00003 to 0.00002, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 17/55\n",
      "3533/3533 [==============================] - 714s 202ms/step - loss: 2.0225e-05 - val_loss: 2.4643e-05\n",
      "\n",
      "Epoch 00017: loss improved from 0.00002 to 0.00002, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 18/55\n",
      "3533/3533 [==============================] - 726s 205ms/step - loss: 2.1230e-05 - val_loss: 2.3888e-05\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.00002\n",
      "Epoch 19/55\n",
      "3533/3533 [==============================] - 723s 205ms/step - loss: 2.1567e-05 - val_loss: 1.6576e-05\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.00002\n",
      "Epoch 20/55\n",
      "3533/3533 [==============================] - 721s 204ms/step - loss: 1.7372e-05 - val_loss: 1.8018e-05\n",
      "\n",
      "Epoch 00020: loss improved from 0.00002 to 0.00002, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 21/55\n",
      "3533/3533 [==============================] - 721s 204ms/step - loss: 1.8695e-05 - val_loss: 2.0040e-05\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.00002\n",
      "Epoch 22/55\n",
      "3533/3533 [==============================] - 721s 204ms/step - loss: 1.6590e-05 - val_loss: 1.5941e-05\n",
      "\n",
      "Epoch 00022: loss improved from 0.00002 to 0.00002, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 23/55\n",
      "3533/3533 [==============================] - 718s 203ms/step - loss: 1.5932e-05 - val_loss: 1.6481e-05\n",
      "\n",
      "Epoch 00023: loss improved from 0.00002 to 0.00002, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 24/55\n",
      "3533/3533 [==============================] - 736s 208ms/step - loss: 1.9637e-05 - val_loss: 1.4436e-05\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.00002\n",
      "Epoch 25/55\n",
      "3533/3533 [==============================] - 706s 200ms/step - loss: 1.4599e-05 - val_loss: 1.5358e-05\n",
      "\n",
      "Epoch 00025: loss improved from 0.00002 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 26/55\n",
      "3533/3533 [==============================] - 718s 203ms/step - loss: 1.6787e-05 - val_loss: 1.3677e-05\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.00001\n",
      "Epoch 27/55\n",
      "3533/3533 [==============================] - 722s 204ms/step - loss: 1.4153e-05 - val_loss: 1.9746e-05\n",
      "\n",
      "Epoch 00027: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 28/55\n",
      "3533/3533 [==============================] - 721s 204ms/step - loss: 2.4281e-05 - val_loss: 1.2438e-05\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.00001\n",
      "Epoch 29/55\n",
      "3533/3533 [==============================] - 715s 202ms/step - loss: 1.3828e-05 - val_loss: 2.0336e-05\n",
      "\n",
      "Epoch 00029: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 30/55\n",
      "3533/3533 [==============================] - 716s 203ms/step - loss: 1.4961e-05 - val_loss: 1.1932e-05\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.00001\n",
      "Epoch 31/55\n",
      "3533/3533 [==============================] - 716s 203ms/step - loss: 1.3008e-05 - val_loss: 1.1312e-05\n",
      "\n",
      "Epoch 00031: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 32/55\n",
      "3533/3533 [==============================] - 719s 204ms/step - loss: 1.2647e-05 - val_loss: 1.1292e-05\n",
      "\n",
      "Epoch 00032: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 33/55\n",
      "3533/3533 [==============================] - 717s 203ms/step - loss: 1.3956e-05 - val_loss: 1.1353e-05\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.00001\n",
      "Epoch 34/55\n",
      "3533/3533 [==============================] - 716s 203ms/step - loss: 1.3724e-05 - val_loss: 3.2781e-04\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.00001\n",
      "Epoch 35/55\n",
      "3533/3533 [==============================] - 718s 203ms/step - loss: 1.1299e-05 - val_loss: 1.0127e-05\n",
      "\n",
      "Epoch 00035: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 36/55\n",
      "3533/3533 [==============================] - 721s 204ms/step - loss: 1.1647e-05 - val_loss: 1.1134e-05\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.00001\n",
      "Epoch 37/55\n",
      "3533/3533 [==============================] - 723s 205ms/step - loss: 1.1260e-05 - val_loss: 1.2345e-05\n",
      "\n",
      "Epoch 00037: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 38/55\n",
      "3533/3533 [==============================] - 721s 204ms/step - loss: 1.2685e-05 - val_loss: 1.0365e-05\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.00001\n",
      "Epoch 39/55\n",
      "3533/3533 [==============================] - 714s 202ms/step - loss: 1.0842e-05 - val_loss: 2.9326e-05\n",
      "\n",
      "Epoch 00039: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 40/55\n",
      "3533/3533 [==============================] - 711s 201ms/step - loss: 1.0713e-05 - val_loss: 3.2741e-05\n",
      "\n",
      "Epoch 00040: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 41/55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3533/3533 [==============================] - 714s 202ms/step - loss: 1.0972e-05 - val_loss: 9.0697e-06\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.00001\n",
      "Epoch 42/55\n",
      "3533/3533 [==============================] - 718s 203ms/step - loss: 1.0450e-05 - val_loss: 9.0018e-06\n",
      "\n",
      "Epoch 00042: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 43/55\n",
      "3533/3533 [==============================] - 721s 204ms/step - loss: 1.0255e-05 - val_loss: 2.6829e-05\n",
      "\n",
      "Epoch 00043: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 44/55\n",
      "3533/3533 [==============================] - 716s 203ms/step - loss: 1.0038e-05 - val_loss: 9.9061e-06\n",
      "\n",
      "Epoch 00044: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 45/55\n",
      "3533/3533 [==============================] - 713s 202ms/step - loss: 1.2075e-05 - val_loss: 8.6925e-06\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.00001\n",
      "Epoch 46/55\n",
      "3533/3533 [==============================] - 715s 202ms/step - loss: 9.6726e-06 - val_loss: 1.2617e-05\n",
      "\n",
      "Epoch 00046: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 47/55\n",
      "3533/3533 [==============================] - 719s 204ms/step - loss: 9.7503e-06 - val_loss: 9.4183e-06\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00001\n",
      "Epoch 48/55\n",
      "3533/3533 [==============================] - 722s 204ms/step - loss: 9.7396e-06 - val_loss: 8.3517e-06\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.00001\n",
      "Epoch 49/55\n",
      "3533/3533 [==============================] - 723s 205ms/step - loss: 9.6215e-06 - val_loss: 8.9053e-06\n",
      "\n",
      "Epoch 00049: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 50/55\n",
      "3533/3533 [==============================] - 724s 205ms/step - loss: 1.0528e-05 - val_loss: 9.0542e-06\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.00001\n",
      "Epoch 51/55\n",
      "3533/3533 [==============================] - 723s 205ms/step - loss: 9.2341e-06 - val_loss: 9.8693e-06\n",
      "\n",
      "Epoch 00051: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 52/55\n",
      "3533/3533 [==============================] - 720s 204ms/step - loss: 9.1682e-06 - val_loss: 1.2865e-05\n",
      "\n",
      "Epoch 00052: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 53/55\n",
      "3533/3533 [==============================] - 722s 204ms/step - loss: 9.1436e-06 - val_loss: 8.8626e-06\n",
      "\n",
      "Epoch 00053: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 54/55\n",
      "3533/3533 [==============================] - 728s 206ms/step - loss: 9.0265e-06 - val_loss: 8.8013e-06\n",
      "\n",
      "Epoch 00054: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n",
      "Epoch 55/55\n",
      "3533/3533 [==============================] - 738s 209ms/step - loss: 8.9815e-06 - val_loss: 8.0276e-06\n",
      "\n",
      "Epoch 00055: loss improved from 0.00001 to 0.00001, saving model to models\\small_autoencoder_check_point.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    auto_encoder_day.fit(train_data_oneDayChart, train_data_oneDayChart, epochs=55, batch_size=64, callbacks=[checkpoint], validation_data=(test_data_oneDayChart, test_data_oneDayChart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd7d13ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[32,64,28,44] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_14/model_13/conv2d_transpose_22/conv2d_transpose (defined at \\AppData\\Local\\Temp/ipykernel_10368/1231674141.py:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[model_14/model_13/conv2d_transpose_24/Tanh/_113]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[32,64,28,44] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_14/model_13/conv2d_transpose_22/conv2d_transpose (defined at \\AppData\\Local\\Temp/ipykernel_10368/1231674141.py:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_predict_function_271142]\n\nFunction call stack:\npredict_function -> predict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10368/1231674141.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0men_train_data_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauto_encoder_day\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_oneDayChart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0men_test_data_day\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mauto_encoder_day\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_oneDayChart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1749\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1751\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1752\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[32,64,28,44] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_14/model_13/conv2d_transpose_22/conv2d_transpose (defined at \\AppData\\Local\\Temp/ipykernel_10368/1231674141.py:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[model_14/model_13/conv2d_transpose_24/Tanh/_113]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[32,64,28,44] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_14/model_13/conv2d_transpose_22/conv2d_transpose (defined at \\AppData\\Local\\Temp/ipykernel_10368/1231674141.py:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_predict_function_271142]\n\nFunction call stack:\npredict_function -> predict_function\n"
     ]
    }
   ],
   "source": [
    "en_train_data_day = auto_encoder_day.predict(train_data_oneDayChart)\n",
    "en_test_data_day =  auto_encoder_day.predict(test_data_oneDayChart)\n",
    "\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "775753fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "auto_encoder_day.save('models/auto_encoder_output_oneDayChart_28_44_4.h5')\n",
    "encoder_day.save('models/encoder_output_oneDayChart_28_44_4.h5')\n",
    "decoder_day.save('models/decoder/decoder_output_oneDayChart_28_44_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4b7b71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_for_dnn(day,week,month,next_day):\n",
    "\n",
    "    encoder_day =  load_model('models/encoder/small_encoder_28_44_4.h5')\n",
    "    encoder_week =  load_model('models/decoder/small_decoder_28_44_4.h5')\n",
    "    encoder_month =  load_model('models/decoder/small_decoder_28_44_4.h5')\n",
    "    \n",
    "    day_val = encoder_day.predict(day)\n",
    "    week_val = encoder_week.predict(week)\n",
    "    month_val = encoder_month.predict(month)\n",
    "    \n",
    "    next_day = encoder_day.predict(next_day)\n",
    "    \n",
    "    return list(day_val+ week_val+ month_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3db6804d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-dc088cc683ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_input_day = Input(shape=(1,10))\n",
    "x_day = keras.layers.Dense(10, activation='relu')(encoder_input_day)\n",
    "x_day = keras.layers.Dense(6, activation='relu')(x_day)\n",
    "x_month = Dense(4, activation='relu')(x_day)\n",
    "x_month = Dense(4)(x_day)\n",
    "\n",
    "model_dnn = Model(inputs= encoder_input_day,outputs = x_month)\n",
    "\n",
    "\n",
    "history = model_dnn.fit(df[:,0:4],df[:,-1],steps_per_epoch = 10 ,epochs = 700,validation_data = (df_test[:,0:4],df_test[:,-1]),validation_steps =15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "99558439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Inputs to a layer should be tensors. Got: <keras.layers.merge.Concatenate object at 0x7f7e25c21130>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-8d571ac1c39b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# have a `shape` attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Inputs to a layer should be tensors. Got: {x}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got: <keras.layers.merge.Concatenate object at 0x7f7e25c21130>"
     ]
    }
   ],
   "source": [
    "# 5번과 7번 분류 65% 추세 넣어서 \n",
    "# 5번과 7번 분류 70% 추세 안넣어서 import_train 으로 \n",
    "import keras\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "encoder_input_day = Input(shape=(1,4))\n",
    "encoder_input_week = Input(shape=(1,3))\n",
    "encoder_input_month = Input(shape=(1,3))\n",
    "\n",
    "\n",
    "\n",
    "x_day = keras.layers.Dense(10, activation='relu')(encoder_input_day)\n",
    "x_week = keras.layers.Dense(10, activation='relu')(encoder_input_week)\n",
    "x_month = Dense(14, activation='relu')(encoder_input_month)\n",
    "\n",
    "\n",
    "inu = Concatenate([x_day,x_week,x_month])\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Dense(1, activation='relu')(inu)\n",
    "output = tf.keras.layers.Dense(4)(x)\n",
    "\n",
    "# model_dnn.compile(optimizer='adam',\n",
    "#               loss='mse',\n",
    "#              )\n",
    "\n",
    "\n",
    "# model_dnn = Model(inputs=[encoder_input_day,encoder_input_week,encoder_input_month],outputs = output)\n",
    "\n",
    "# history = model_dnn.fit(np.array(p),np.array(xy[:,1]),steps_per_epoch = 10 ,epochs = 700,validation_data = (np.array(p_k),np.array(vr[:,1])),validation_steps =15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c98bc",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e21e9666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7e2e85f850>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(testX[3]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e4db4ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7e2eb01e20>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(test_data[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d7c739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 2584 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A046D39160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[ -459.4101    -1014.7185      683.8709       12.1562805]]\n",
      "WARNING:tensorflow:6 out of the last 2585 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A0520924C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a04ea43850>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import PIL.Image as pilimg\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def convertImgToArray(path):\n",
    "    im = np.array(pilimg.open(\"{}\".format(path)))\n",
    "    im = im.reshape(1,28, 44, 3)\n",
    "    return [im]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t = encoder.predict(convertImgToArray('stock_data/image_train/dayChart/dayChart1/000020_20210608.jpg')) \n",
    "p = p[0]\n",
    "plt.imshow(p)\n",
    "\n",
    "p = decoder.predict(t) \n",
    "p = p[0]\n",
    "plt.imshow(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdff80a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "encoder = load_model('models/encoder_day_28_44_4.h5')\n",
    "autoencoder = load_model('models/auto_encoder_day_28_44_4.h5')\n",
    "decoder = load_model('models/decoder_day_28_44_4.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
